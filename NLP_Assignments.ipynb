{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 1"
      ],
      "metadata": {
        "id": "3ZSLxYSoE2OM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmZrsn_UIC1m",
        "outputId": "528c9f7b-18aa-447c-f0a9-7ef4ee5e5265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia-api\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk import NaiveBayesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0j_4EtwIE1i",
        "outputId": "2d8be401-2692-446f-8aef-49f13d016164"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lists of terms\n",
        "location_terms = [\"region\", \"area\", \"terrain\", \"nation\", \"town\"]\n",
        "non_location_terms = [\"innovation\", \"knowledge\", \"machine learning\", \"computing\"]\n",
        "\n",
        "def get_wiki_content_with_terms(topic, terms_list):\n",
        "    # Initialize Wikipedia API object\n",
        "    wiki_api = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, headers={'User-Agent': 'Farid_Tavakkolinia'})\n",
        "\n",
        "    # Get the Wikipedia page for the given topic\n",
        "    wiki_page = wiki_api.page(topic)\n",
        "\n",
        "    if wiki_page.exists():\n",
        "        # Obtain the text content of the page\n",
        "        content = wiki_page.text\n",
        "\n",
        "        # Find terms within the text\n",
        "        found_terms = [term for term in terms_list if term.lower() in content.lower()]\n",
        "\n",
        "        return content, found_terms\n",
        "    else:\n",
        "        print(f\"The page for '{topic}' does not exist.\")\n",
        "        return None, None\n",
        "\n",
        "# Retrieve Wikipedia page content with terms\n",
        "topic = \"Machine learning\"\n",
        "content, found_terms = get_wiki_content_with_terms(topic, non_location_terms)\n",
        "\n",
        "if content:\n",
        "    print(f\"Content for '{topic}':\")\n",
        "    print(content[:500])  # Display the first 500 characters of the content\n",
        "    print(\"Found Terms:\", found_terms)\n",
        "\n",
        "def process_text_with_terms(text, terms, stopwords_set=None, text_stemmer=None, text_lemmatizer=None):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Use provided stopwords set or default to NLTK stop words\n",
        "    stopwords_set = stopwords_set or set(stopwords.words('english'))\n",
        "\n",
        "    # Filter out stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords_set]\n",
        "\n",
        "    # Use provided stemmer or default to Porter Stemmer\n",
        "    text_stemmer = text_stemmer or PorterStemmer()\n",
        "    stemmed_tokens = [text_stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Use provided lemmatizer or default to WordNet Lemmatizer\n",
        "    text_lemmatizer = text_lemmatizer or WordNetLemmatizer()\n",
        "    lemmatized_tokens = [text_lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    # Reconstruct the preprocessed text\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    # Append terms to the processed text\n",
        "    processed_text += ' ' + ' '.join(terms)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Use Bag of Words without preprocessing\n",
        "bow_text = content.lower()\n",
        "bow_features = {word: True for word in word_tokenize(bow_text)}\n",
        "\n",
        "# Use Snowball stemmer and custom stopwords\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "processed_text_with_snowball = process_text_with_terms(content, found_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer)\n",
        "\n",
        "# Use WordNet lemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "processed_text_with_lemmatizer = process_text_with_terms(content, found_terms, stopwords_set=set(stopwords.words('english')), text_lemmatizer=wordnet_lemmatizer)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nBag of Words without preprocessing:\")\n",
        "print(bow_features)\n",
        "\n",
        "print(\"\\nProcessed Text with Snowball stopwords and stemmer:\")\n",
        "print(processed_text_with_snowball[:500])\n",
        "\n",
        "print(\"\\nProcessed Text with WordNet Lemmatizer:\")\n",
        "print(processed_text_with_lemmatizer[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGTDDHn5IE5B",
        "outputId": "7dba2552-0bd6-48b3-f979-5951d0c1a61c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content for 'Machine learning':\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\n",
            "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and med\n",
            "Found Terms: ['knowledge', 'machine learning', 'computing']\n",
            "\n",
            "Bag of Words without preprocessing:\n",
            "{'machine': True, 'learning': True, '(': True, 'ml': True, ')': True, 'is': True, 'a': True, 'field': True, 'of': True, 'study': True, 'in': True, 'artificial': True, 'intelligence': True, 'concerned': True, 'with': True, 'the': True, 'development': True, 'and': True, 'statistical': True, 'algorithms': True, 'that': True, 'can': True, 'learn': True, 'from': True, 'data': True, 'generalize': True, 'to': True, 'unseen': True, ',': True, 'thus': True, 'perform': True, 'tasks': True, 'without': True, 'explicit': True, 'instructions': True, '.': True, 'recently': True, 'neural': True, 'networks': True, 'have': True, 'been': True, 'able': True, 'surpass': True, 'many': True, 'previous': True, 'approaches': True, 'performance': True, 'finds': True, 'application': True, 'fields': True, 'including': True, 'natural': True, 'language': True, 'processing': True, 'computer': True, 'vision': True, 'speech': True, 'recognition': True, 'email': True, 'filtering': True, 'agriculture': True, 'medicine': True, 'when': True, 'applied': True, 'business': True, 'problems': True, 'it': True, 'known': True, 'under': True, 'name': True, 'predictive': True, 'analytics': True, 'although': True, 'not': True, 'all': True, 'statistically': True, 'based': True, 'computational': True, 'statistics': True, 'an': True, 'important': True, 'source': True, \"'s\": True, 'methods': True, 'mathematical': True, 'foundations': True, 'are': True, 'provided': True, 'by': True, 'optimization': True, 'programming': True, 'mining': True, 'related': True, 'parallel': True, 'focusing': True, 'on': True, 'exploratory': True, 'analysis': True, 'eda': True, 'through': True, 'unsupervised': True, 'theoretical': True, 'viewpoint': True, 'probably': True, 'approximately': True, 'correct': True, 'pac': True, 'provides': True, 'framework': True, 'for': True, 'describing': True, 'history': True, 'term': True, 'was': True, 'coined': True, '1959': True, 'arthur': True, 'samuel': True, 'ibm': True, 'employee': True, 'pioneer': True, 'gaming': True, 'synonym': True, 'self-teaching': True, 'computers': True, 'also': True, 'used': True, 'this': True, 'time': True, 'period': True, 'earliest': True, 'model': True, 'introduced': True, '1950s': True, 'invented': True, 'program': True, 'calculated': True, 'winning': True, 'chance': True, 'checkers': True, 'each': True, 'side': True, 'roots': True, 'back': True, 'decades': True, 'human': True, 'desire': True, 'effort': True, 'cognitive': True, 'processes': True, '1949': True, 'canadian': True, 'psychologist': True, 'donald': True, 'hebb': True, 'published': True, 'book': True, 'organization': True, 'behavior': True, 'which': True, 'he': True, 'structure': True, 'formed': True, 'certain': True, 'interactions': True, 'among': True, 'nerve': True, 'cells': True, 'neurons': True, 'interacting': True, 'one': True, 'another': True, 'set': True, 'groundwork': True, 'how': True, 'ais': True, 'work': True, 'nodes': True, 'or': True, 'communicate': True, 'other': True, 'researchers': True, 'who': True, 'studied': True, 'systems': True, 'contributed': True, 'modern': True, 'technologies': True, 'as': True, 'well': True, 'logician': True, 'walter': True, 'pitts': True, 'warren': True, 'mcculloch': True, 'proposed': True, 'early': True, 'models': True, 'come': True, 'up': True, 'mirror': True, 'thought': True, '1960s': True, 'experimental': True, '``': True, \"''\": True, 'punched': True, 'tape': True, 'memory': True, 'called': True, 'cybertron': True, 'had': True, 'developed': True, 'raytheon': True, 'company': True, 'analyze': True, 'sonar': True, 'signals': True, 'electrocardiograms': True, 'patterns': True, 'using': True, 'rudimentary': True, 'reinforcement': True, 'repetitively': True, 'trained': True, 'operator/teacher': True, 'recognize': True, 'equipped': True, 'goof': True, 'button': True, 'cause': True, 'reevaluate': True, 'incorrect': True, 'decisions': True, 'representative': True, 'research': True, 'into': True, 'during': True, 'nilsson': True, 'machines': True, 'dealing': True, 'mostly': True, 'pattern': True, 'classification': True, 'interest': True, 'continued': True, '1970s': True, 'described': True, 'duda': True, 'hart': True, '1973.': True, '1981': True, 'report': True, 'given': True, 'teaching': True, 'strategies': True, 'so': True, 'network': True, 'learns': True, '40': True, 'characters': True, '26': True, 'letters': True, '10': True, 'digits': True, '4': True, 'special': True, 'symbols': True, 'terminal': True, 'tom': True, 'm.': True, 'mitchell': True, 'widely': True, 'quoted': True, 'more': True, 'formal': True, 'definition': True, ':': True, 'said': True, 'experience': True, 'e': True, 'respect': True, 'some': True, 'class': True, 't': True, 'measure': True, 'p': True, 'if': True, 'its': True, 'at': True, 'measured': True, 'improves': True, 'e.': True, 'offers': True, 'fundamentally': True, 'operational': True, 'rather': True, 'than': True, 'defining': True, 'terms': True, 'follows': True, 'alan': True, 'turing': True, 'proposal': True, 'his': True, 'paper': True, 'computing': True, 'machinery': True, 'question': True, 'think': True, '?': True, 'replaced': True, 'do': True, 'what': True, 'we': True, 'thinking': True, 'entities': True, 'modern-day': True, 'has': True, 'two': True, 'objectives': True, 'classify': True, ';': True, 'purpose': True, 'make': True, 'predictions': True, 'future': True, 'outcomes': True, 'these': True, 'hypothetical': True, 'algorithm': True, 'specific': True, 'classifying': True, 'may': True, 'use': True, 'moles': True, 'coupled': True, 'supervised': True, 'order': True, 'train': True, 'cancerous': True, 'stock': True, 'trading': True, 'inform': True, 'trader': True, 'potential': True, 'relationships': True, 'scientific': True, 'endeavor': True, 'grew': True, 'out': True, 'quest': True, 'ai': True, 'days': True, 'academic': True, 'discipline': True, 'were': True, 'interested': True, 'having': True, 'they': True, 'attempted': True, 'approach': True, 'problem': True, 'various': True, 'symbolic': True, 'then': True, 'termed': True, 'perceptrons': True, 'later': True, 'found': True, 'be': True, 'reinventions': True, 'generalized': True, 'linear': True, 'probabilistic': True, 'reasoning': True, 'employed': True, 'especially': True, 'automated': True, 'medical': True, 'diagnosis': True, '488': True, 'however': True, 'increasing': True, 'emphasis': True, 'logical': True, 'knowledge-based': True, 'caused': True, 'rift': True, 'between': True, 'plagued': True, 'practical': True, 'acquisition': True, 'representation': True, '1980': True, 'expert': True, 'dominate': True, 'favor': True, 'symbolic/knowledge-based': True, 'did': True, 'continue': True, 'within': True, 'leading': True, 'inductive': True, 'logic': True, 'ilp': True, 'but': True, 'line': True, 'now': True, 'outside': True, 'proper': True, 'information': True, 'retrieval': True, '708–710': True, '755': True, 'abandoned': True, 'science': True, 'around': True, 'same': True, 'too': True, 'ai/cs': True, 'connectionism': True, 'disciplines': True, 'hopfield': True, 'rumelhart': True, 'hinton': True, 'their': True, 'main': True, 'success': True, 'came': True, 'mid-1980s': True, 'reinvention': True, 'backpropagation': True, '25': True, 'reorganized': True, 'recognized': True, 'own': True, 'started': True, 'flourish': True, '1990s': True, 'changed': True, 'goal': True, 'achieving': True, 'tackling': True, 'solvable': True, 'nature': True, 'shifted': True, 'focus': True, 'away': True, 'inherited': True, 'toward': True, 'borrowed': True, 'fuzzy': True, 'probability': True, 'theory': True, 'compression': True, 'often': True, 'employ': True, 'overlap': True, 'significantly': True, 'while': True, 'focuses': True, 'prediction': True, 'properties': True, 'learned': True, 'training': True, 'discovery': True, 'previously': True, 'unknown': True, 'step': True, 'knowledge': True, 'databases': True, 'uses': True, 'different': True, 'goals': True, 'hand': True, 'employs': True, 'preprocessing': True, 'improve': True, 'learner': True, 'accuracy': True, 'much': True, 'confusion': True, 'communities': True, 'separate': True, 'conferences': True, 'journals': True, 'ecml': True, 'pkdd': True, 'being': True, 'major': True, 'exception': True, 'comes': True, 'basic': True, 'assumptions': True, 'usually': True, 'evaluated': True, 'ability': True, 'reproduce': True, 'kdd': True, 'key': True, 'task': True, 'uninformed': True, 'method': True, 'will': True, 'easily': True, 'outperformed': True, 'typical': True, 'due': True, 'unavailability': True, 'intimate': True, 'ties': True, 'formulated': True, 'minimization': True, 'loss': True, 'function': True, 'examples': True, 'functions': True, 'express': True, 'discrepancy': True, 'actual': True, 'instances': True, 'example': True, 'wants': True, 'assign': True, 'label': True, 'correctly': True, 'predict': True, 'preassigned': True, 'labels': True, 'generalization': True, 'difference': True, 'arises': True, 'minimize': True, 'minimizing': True, 'samples': True, 'characterizing': True, 'active': True, 'topic': True, 'current': True, 'deep': True, 'closely': True, 'distinct': True, 'principal': True, 'draws': True, 'population': True, 'inferences': True, 'sample': True, 'generalizable': True, 'according': True, 'michael': True, 'i.': True, 'jordan': True, 'ideas': True, 'methodological': True, 'principles': True, 'tools': True, 'long': True, 'pre-history': True, 'suggested': True, 'placeholder': True, 'call': True, 'overall': True, 'conventional': True, 'analyses': True, 'require': True, 'priori': True, 'selection': True, 'most': True, 'suitable': True, 'addition': True, 'only': True, 'significant': True, 'theoretically': True, 'relevant': True, 'variables': True, 'included': True, 'contrast': True, 'built': True, 'pre-structured': True, 'shape': True, 'detecting': True, 'underlying': True, 'input': True, 'accurate': True, 'ultimate': True, 'leo': True, 'breiman': True, 'distinguished': True, 'modeling': True, 'paradigms': True, 'algorithmic': True, 'wherein': True, 'means': True, 'less': True, 'like': True, 'random': True, 'forest': True, 'statisticians': True, 'adopted': True, 'combined': True, 'physics': True, 'analytical': True, 'techniques': True, 'derived': True, 'deep-rooted': True, 'disordered': True, 'extended': True, 'large-scale': True, 'e.g.': True, 'weight': True, 'space': True, 'finding': True, 'applications': True, 'area': True, 'diagnostics': True, 'core': True, 'objective': True, 'context': True, 'accurately': True, 'new': True, 'examples/tasks': True, 'after': True, 'experienced': True, 'generally': True, 'distribution': True, 'considered': True, 'occurrences': True, 'build': True, 'general': True, 'about': True, 'enables': True, 'produce': True, 'sufficiently': True, 'cases': True, 'branch': True, 'via': True, 'because': True, 'sets': True, 'finite': True, 'uncertain': True, 'does': True, 'yield': True, 'guarantees': True, 'instead': True, 'bounds': True, 'quite': True, 'common': True, 'bias–variance': True, 'decomposition': True, 'way': True, 'quantify': True, 'error': True, 'best': True, 'complexity': True, 'hypothesis': True, 'should': True, 'match': True, 'complex': True, 'fitted': True, 'increased': True, 'response': True, 'decreases': True, 'subject': True, 'overfitting': True, 'poorer': True, 'theorists': True, 'feasibility': True, 'computation': True, 'feasible': True, 'done': True, 'polynomial': True, 'there': True, 'kinds': True, 'results': True, 'positive': True, 'show': True, 'negative': True, 'classes': True, 'traditionally': True, 'divided': True, 'three': True, 'broad': True, 'categories': True, 'correspond': True, 'depending': True, 'signal': True, 'feedback': True, 'available': True, 'system': True, 'presented': True, 'inputs': True, 'desired': True, 'outputs': True, 'teacher': True, 'rule': True, 'maps': True, 'no': True, 'leaving': True, 'find': True, 'itself': True, 'discovering': True, 'hidden': True, 'towards': True, 'end': True, 'feature': True, 'interacts': True, 'dynamic': True, 'environment': True, 'must': True, 'such': True, 'driving': True, 'vehicle': True, 'playing': True, 'game': True, 'against': True, 'opponent': True, 'navigates': True, 'analogous': True, 'rewards': True, 'tries': True, 'maximize': True, 'advantages': True, 'limitations': True, 'single': True, 'works': True, 'contains': True, 'both': True, 'consists': True, 'output': True, 'supervisory': True, 'represented': True, 'array': True, 'vector': True, 'sometimes': True, 'matrix': True, 'iterative': True, 'associated': True, 'optimal': True, 'allows': True, 'determine': True, 'part': True, 'over': True, 'types': True, 'supervised-learning': True, 'include': True, 'regression': True, 'restricted': True, 'limited': True, 'values': True, 'any': True, 'numerical': True, 'value': True, 'range': True, 'filters': True, 'emails': True, 'would': True, 'incoming': True, 'folder': True, 'file': True, 'similarity': True, 'measures': True, 'similar': True, 'objects': True, 'ranking': True, 'recommendation': True, 'visual': True, 'identity': True, 'tracking': True, 'face': True, 'verification': True, 'speaker': True, 'structures': True, 'labeled': True, 'classified': True, 'categorized': True, 'responding': True, 'identify': True, 'commonalities': True, 'react': True, 'presence': True, 'absence': True, 'piece': True, 'central': True, 'clustering': True, 'dimensionality': True, 'reduction': True, 'density': True, 'estimation': True, 'streamlined': True, 'process': True, 'identifying': True, 'large': True, 'indel': True, 'haplotypes': True, 'gene': True, 'pan-genome': True, 'cluster': True, 'assignment': True, 'observations': True, 'subsets': True, 'clusters': True, 'predesignated': True, 'criteria': True, 'drawn': True, 'dissimilar': True, 'defined': True, 'metric': True, 'internal': True, 'compactness': True, 'members': True, 'separation': True, 'estimated': True, 'graph': True, 'connectivity': True, 'semi-supervised': True, 'falls': True, 'completely': True, 'missing': True, 'yet': True, 'machine-learning': True, 'unlabeled': True, 'conjunction': True, 'small': True, 'amount': True, 'considerable': True, 'improvement': True, 'weakly': True, 'noisy': True, 'imprecise': True, 'cheaper': True, 'obtain': True, 'resulting': True, 'larger': True, 'effective': True, 'software': True, 'agents': True, 'ought': True, 'take': True, 'actions': True, 'notion': True, 'cumulative': True, 'reward': True, 'generality': True, 'control': True, 'operations': True, 'simulation-based': True, 'multi-agent': True, 'swarm': True, 'genetic': True, 'typically': True, 'markov': True, 'decision': True, 'mdp': True, 'reinforcements': True, 'assume': True, 'exact': True, 'infeasible': True, 'autonomous': True, 'vehicles': True, 'play': True, 'reducing': True, 'number': True, 'consideration': True, 'obtaining': True, 'words': True, 'dimension': True, 'features': True, 'either': True, 'elimination': True, 'extraction': True, 'popular': True, 'component': True, 'pca': True, 'involves': True, 'changing': True, 'higher-dimensional': True, '3d': True, 'smaller': True, '2d': True, 'keeping': True, 'original': True, 'manifold': True, 'proposes': True, 'high-dimensional': True, 'lie': True, 'along': True, 'low-dimensional': True, 'manifolds': True, 'assumption': True, 'regularization': True, 'fit': True, 'neatly': True, 'three-fold': True, 'categorization': True, 'meta-learning': True, 'self-learning': True, 'paradigm': True, '1982': True, 'capable': True, 'named': True, 'crossbar': True, 'adaptive': True, 'caa': True, 'external': True, 'advice': True, 'computes': True, 'fashion': True, 'emotions': True, 'feelings': True, 'consequence': True, 'situations': True, 'driven': True, 'interaction': True, 'cognition': True, 'emotion': True, 'updates': True, 'w': True, '=||w': True, 's': True, '||': True, 'iteration': True, 'executes': True, 'following': True, 'routine': True, 'situation': True, 'action': True, 'receive': True, \"s'\": True, 'compute': True, 'v': True, \"'\": True, 'update': True, '=': True, '+': True, 'a.': True, 'neither': True, 'nor': True, 'backpropagated': True, 'secondary': True, 'exists': True, 'environments': True, 'behavioral': True, 'where': True, 'behaves': True, 'wherefrom': True, 'initially': True, 'once': True, 'receives': True, 'initial': True, 'encountered': True, 'receiving': True, 'genome': True, 'species': True, 'goal-seeking': True, 'desirable': True, 'undesirable': True, 'several': True, 'aim': True, 'better': True, 'representations': True, 'classic': True, 'attempt': True, 'preserve': True, 'transform': True, 'makes': True, 'useful': True, 'pre-processing': True, 'before': True, 'performing': True, 'technique': True, 'reconstruction': True, 'coming': True, 'data-generating': True, 'necessarily': True, 'faithful': True, 'configurations': True, 'implausible': True, 'replaces': True, 'manual': True, 'engineering': True, 'them': True, 'multilayer': True, 'dictionary': True, 'independent': True, 'autoencoders': True, 'factorization': True, 'forms': True, 'constraint': True, 'sparse': True, 'coding': True, 'meaning': True, 'zeros': True, 'multilinear': True, 'subspace': True, 'directly': True, 'tensor': True, 'multidimensional': True, 'reshaping': True, 'vectors': True, 'discover': True, 'multiple': True, 'levels': True, 'hierarchy': True, 'higher-level': True, 'abstract': True, 'generating': True, 'lower-level': True, 'argued': True, 'intelligent': True, 'disentangles': True, 'factors': True, 'variation': True, 'explain': True, 'observed': True, 'motivated': True, 'fact': True, 'mathematically': True, 'computationally': True, 'convenient': True, 'real-world': True, 'images': True, 'video': True, 'sensory': True, 'yielded': True, 'attempts': True, 'algorithmically': True, 'define': True, 'alternative': True, 'examination': True, 'relying': True, 'combination': True, 'basis': True, 'assumed': True, 'strongly': True, 'np-hard': True, 'difficult': True, 'solve': True, 'heuristic': True, 'k-svd': True, 'contexts': True, 'belongs': True, 'already': True, 'sparsely': True, 'corresponding': True, 'image': True, 'de-noising': True, 'idea': True, 'clean': True, 'patch': True, 'noise': True, 'anomaly': True, 'detection': True, 'outlier': True, 'identification': True, 'rare': True, 'items': True, 'events': True, 'raise': True, 'suspicions': True, 'differing': True, 'majority': True, 'anomalous': True, 'represent': True, 'issue': True, 'bank': True, 'fraud': True, 'structural': True, 'defect': True, 'errors': True, 'text': True, 'anomalies': True, 'referred': True, 'outliers': True, 'novelties': True, 'deviations': True, 'exceptions': True, 'particular': True, 'abuse': True, 'intrusion': True, 'interesting': True, 'unexpected': True, 'bursts': True, 'inactivity': True, 'adhere': True, 'object': True, 'fail': True, 'unless': True, 'aggregated': True, 'appropriately': True, 'detect': True, 'micro-clusters': True, 'exist': True, 'test': True, 'normal': True, 'looking': True, 'seem': True, 'least': True, 'remainder': True, 'abnormal': True, 'classifier': True, 'inherently': True, 'unbalanced': True, 'construct': True, 'representing': True, 'likelihood': True, 'instance': True, 'generated': True, 'robot': True, 'inspired': True, 'multitude': True, 'starting': True, 'finally': True, 'e.g': True, 'maml': True, 'association': True, 'rules': True, 'rule-based': True, 'intended': True, 'strong': True, 'discovered': True, 'interestingness': True, 'identifies': True, 'evolves': True, 'store': True, 'manipulate': True, 'apply': True, 'characteristic': True, 'utilization': True, 'relational': True, 'collectively': True, 'captured': True, 'commonly': True, 'singular': True, 'universally': True, 'immune': True, 'concept': True, 'rakesh': True, 'agrawal': True, 'tomasz': True, 'imieliński': True, 'arun': True, 'swami': True, 'regularities': True, 'products': True, 'transaction': True, 'recorded': True, 'point-of-sale': True, 'pos': True, 'supermarkets': True, '{': True, 'o': True, 'n': True, 'i': True, '}': True, '⇒': True, 'b': True, 'u': True, 'r': True, 'g': True, '\\\\displaystyle': True, '\\\\': True, '\\\\mathrm': True, 'onions': True, 'potatoes': True, '\\\\rightarrow': True, 'burger': True, 'sales': True, 'supermarket': True, 'indicate': True, 'customer': True, 'buys': True, 'together': True, 'likely': True, 'buy': True, 'hamburger': True, 'meat': True, 'marketing': True, 'activities': True, 'promotional': True, 'pricing': True, 'product': True, 'placements': True, 'market': True, 'basket': True, 'today': True, 'areas': True, 'web': True, 'usage': True, 'continuous': True, 'production': True, 'bioinformatics': True, 'sequence': True, 'consider': True, 'across': True, 'transactions': True, 'lcs': True, 'family': True, 'combine': True, 'seek': True, 'context-dependent': True, 'piecewise': True, 'manner': True, 'uniform': True, 'background': True, 'hypotheses': True, 'encoding': True, 'database': True, 'facts': True, 'derive': True, 'hypothesized': True, 'entails': True, 'considers': True, 'kind': True, 'functional': True, 'programs': True, 'particularly': True, 'gordon': True, 'plotkin': True, 'ehud': True, 'shapiro': True, 'laid': True, 'foundation': True, 'setting': True, 'first': True, 'implementation': True, 'inference': True, 'prolog': True, 'inductively': True, 'inferred': True, 'here': True, 'refers': True, 'philosophical': True, 'induction': True, 'suggesting': True, 'proving': True, 'property': True, 'well-ordered': True, 'type': True, 'dataset': True, 'classifications': True, 'iteratively': True, 'adjusts': True, 'parameters': True, 'extension': True, 'refer': True, 'specificity': True, 'fully': True, 'tuned': True, 'researched': True, 'picking': True, 'anns': True, 'connectionist': True, 'vaguely': True, 'biological': True, 'constitute': True, 'animal': True, 'brains': True, 'considering': True, 'programmed': True, 'task-specific': True, 'ann': True, 'collection': True, 'connected': True, 'units': True, 'loosely': True, 'brain': True, 'connection': True, 'synapses': True, 'transmit': True, 'neuron': True, 'additional': True, 'implementations': True, 'real': True, 'computed': True, 'non-linear': True, 'sum': True, 'connections': True, 'edges': True, 'proceeds': True, 'increases': True, 'strength': True, 'threshold': True, 'sent': True, 'aggregate': True, 'crosses': True, 'layers': True, 'transformations': True, 'travel': True, 'layer': True, 'last': True, 'possibly': True, 'traversing': True, 'times': True, 'attention': True, 'moved': True, 'biology': True, 'variety': True, 'translation': True, 'social': True, 'board': True, 'games': True, 'light': True, 'sound': True, 'hearing': True, 'successful': True, 'trees': True, 'tree': True, 'go': True, 'item': True, 'branches': True, 'conclusions': True, 'target': True, 'leaves': True, 'variable': True, 'discrete': True, 'conjunctions': True, 'lead': True, 'those': True, 'numbers': True, 'visually': True, 'explicitly': True, 'making': True, 'describes': True, 'decision-making': True, 'support-vector': True, 'svms': True, 'marked': True, 'belonging': True, 'svm': True, 'builds': True, 'predicts': True, 'whether': True, 'category': True, 'non-probabilistic': True, 'binary': True, 'platt': True, 'scaling': True, 'efficiently': True, 'kernel': True, 'trick': True, 'implicitly': True, 'mapping': True, 'spaces': True, 'encompasses': True, 'estimate': True, 'relationship': True, 'form': True, 'criterion': True, 'ordinary': True, 'squares': True, 'latter': True, 'mitigate': True, 'bias': True, 'ridge': True, 'go-to': True, 'trendline': True, 'fitting': True, 'microsoft': True, 'excel': True, 'logistic': True, 'even': True, 'introduces': True, 'non-linearity': True, 'taking': True, 'advantage': True, 'map': True, 'bayesian': True, 'belief': True, 'directed': True, 'acyclic': True, 'graphical': True, 'represents': True, 'conditional': True, 'independence': True, 'dag': True, 'could': True, 'diseases': True, 'symptoms': True, 'probabilities': True, 'efficient': True, 'sequences': True, 'protein': True, 'generalizations': True, 'uncertainty': True, 'influence': True, 'diagrams': True, 'gaussian': True, 'stochastic': True, 'every': True, 'multivariate': True, 'relies': True, 'pre-defined': True, 'covariance': True, 'pairs': True, 'points': True, 'relate': True, 'locations': True, 'input–output': True, 'unobserved': True, 'point': True, 'covariances': True, 'surrogate': True, 'hyperparameter': True, 'ga': True, 'search': True, 'mimics': True, 'mutation': True, 'crossover': True, 'generate': True, 'genotypes': True, 'hope': True, 'good': True, 'solutions': True, '1980s': True, 'conversely': True, 'evolutionary': True, 'evidence': True, 'dempster–shafer': True, 'understood': True, 'frameworks': True, 'possibility': True, 'theories': True, 'dempster': True, 'just': True, 'pmf-based': True, 'caveats': True, 'beliefs': True, 'compared': True, 'incorporate': True, 'ignorance': True, 'quantification': True, 'implemented': True, 'domain': True, 'leverage': True, 'fusion': True, 'ensemble': True, 'handle': True, 'boundary': True, 'low': True, 'ambiguous': True, 'issues': True, 'standard': True, 'tend': True, 'difficulty': True, 'resolving': True, 'dependent': True, 'propositions': True, 'higher': True, 'high': True, 'quantity': True, 'reliable': True, 'engineers': True, 'need': True, 'collect': True, 'varied': True, 'corpus': True, 'sensor': True, 'collected': True, 'individual': True, 'users': True, 'service': True, 'something': True, 'watch': True, 'biased': True, 'non-evaluated': True, 'result': True, 'skewed': True, 'undesired': True, 'detrimental': True, 'thereby': True, 'furthering': True, 'impacts': True, 'society': True, 'prepared': True, 'ethics': True, 'becoming': True, 'notably': True, 'integrated': True, 'teams': True, 'federated': True, 'adapted': True, 'distributed': True, 'decentralizes': True, 'allowing': True, 'privacy': True, 'maintained': True, 'needing': True, 'send': True, 'centralized': True, 'server': True, 'efficiency': True, 'decentralizing': True, 'devices': True, 'gboard': True, 'query': True, 'mobile': True, 'phones': True, 'searches': True, 'google': True, '2006': True, 'media-services': True, 'provider': True, 'netflix': True, 'held': True, 'prize': True, 'competition': True, 'user': True, 'preferences': True, 'existing': True, 'cinematch': True, 'movie': True, '%': True, 'joint': True, 'team': True, 'made': True, '&': True, 'labs-research': True, 'collaboration': True, 'big': True, 'chaos': True, 'pragmatic': True, 'win': True, 'grand': True, '2009': True, '$': True, '1': True, 'million': True, 'shortly': True, 'awarded': True, 'realized': True, 'viewers': True, 'ratings': True, 'indicators': True, 'viewing': True, 'everything': True, 'engine': True, 'accordingly': True, '2010': True, 'wall': True, 'street': True, 'journal': True, 'wrote': True, 'firm': True, 'rebellion': True, 'financial': True, 'crisis': True, '2012': True, 'co-founder': True, 'sun': True, 'microsystems': True, 'vinod': True, 'khosla': True, 'predicted': True, '80': True, 'doctors': True, 'jobs': True, 'lost': True, 'next': True, 'diagnostic': True, '2014': True, 'reported': True, 'art': True, 'fine': True, 'paintings': True, 'revealed': True, 'unrecognized': True, 'influences': True, 'artists': True, '2019': True, 'springer': True, 'created': True, '2020': True, 'technology': True, 'help': True, 'diagnoses': True, 'aid': True, 'developing': True, 'cure': True, 'covid-19': True, 'pro-environmental': True, 'travelers': True, 'optimize': True, 'smartphone': True, 'thermal': True, 'phone': True, 'mlas': True, 'utilize': True, 'wide': True, 'characteristics': True, 'returns': True, 'employing': True, 'combining': True, 'forecasts': True, 'far': True, 'obtained': True, 'ols': True, 'recent': True, 'advancements': True, 'quantum': True, 'chemistry': True, 'novel': True, 'enable': True, 'solvent': True, 'effects': True, 'chemical': True, 'reactions': True, 'offering': True, 'chemists': True, 'tailor': True, 'conditions': True, 'tool': True, 'investigate': True, 'evacuation': True, 'scale': True, 'disasters': True, 'tested': True, 'householders': True, 'decide': True, 'evacuate': True, 'wildfires': True, 'hurricanes': True, 'pre': True, 'building': True, 'fires': True, 'transformative': True, 'deliver': True, 'expected': True, 'reasons': True, 'numerous': True, 'lack': True, 'access': True, 'badly': True, 'chosen': True, 'wrong': True, 'people': True, 'resources': True, 'evaluation': True, 'black': True, 'box': True, 'poses': True, 'challenge': True, 'producing': True, 'entirely': True, 'opaque': True, 'coders': True, 'audit': True, 'extracted': True, 'house': True, 'lords': True, 'select': True, 'committee': True, 'claimed': True, '“': True, '”': True, 'substantial': True, 'impact': True, '’': True, 'life': True, 'acceptable': True, 'full': True, 'satisfactory': True, 'explanation': True, '2018': True, 'self-driving': True, 'car': True, 'uber': True, 'failed': True, 'pedestrian': True, 'killed': True, 'collision': True, 'healthcare': True, 'watson': True, 'years': True, 'billions': True, 'dollars': True, 'invested': True, 'bing': True, 'chat': True, 'chatbot': True, 'hostile': True, 'offensive': True, 'strategy': True, 'systematic': True, 'review': True, 'reviewer': True, 'burden': True, 'growth': True, 'biomedical': True, 'literature': True, 'improved': True, 'reduce': True, 'workload': True, 'limiting': True, 'necessary': True, 'sensitivity': True, 'findings': True, 'themselves': True, 'suffer': True, 'biases': True, 'specifically': True, 'customers': True, 'needs': True, 'groups': True, 'human-made': True, 'pick': True, 'constitutional': True, 'unconscious': True, 'present': True, 'shown': True, 'contain': True, 'human-like': True, 'experiment': True, 'carried': True, 'propublica': True, 'investigative': True, 'journalism': True, 'insight': True, 'recidivism': True, 'rates': True, 'prisoners': True, 'falsely': True, 'flagged': True, 'defendants': True, 'risk': True, 'twice': True, 'white': True, '2015': True, 'photos': True, 'tag': True, 'gorillas': True, 'still': True, 'resolved': True, 'reportedly': True, 'workaround': True, 'remove': True, 'recognizing': True, 'non-white': True, '2016': True, 'tay': True, 'twitter': True, 'quickly': True, 'picked': True, 'racist': True, 'sexist': True, 'challenges': True, 'longer': True, 'domains': True, 'concern': True, 'fairness': True, 'propelling': True, 'increasingly': True, 'expressed': True, 'scientists': True, 'fei-fei': True, 'li': True, 'reminds': True, '[': True, ']': True, 'nothing': True, 'and—most': True, 'importantly—it': True, 'powerful': True, 'beginning': True, 'understand': True, 'profound': True, 'responsibility': True, 'explainability': True, 'explainable': True, 'xai': True, 'interpretable': True, 'xml': True, 'humans': True, 'contrasts': True, 'designers': True, 'why': True, 'arrived': True, 'refining': True, 'mental': True, 'ai-powered': True, 'dismantling': True, 'misconceptions': True, 'promises': True, 'effectively': True, 'right': True, 'settling': True, 'bad': True, 'overly': True, 'gerrymandered': True, 'past': True, 'rewarding': True, 'accordance': True, 'fits': True, 'penalizing': True, 'vulnerabilities': True, 'learners': True, 'disappoint': True, 'lesson': True, 'toy': True, 'pictures': True, 'brown': True, 'horses': True, 'cats': True, 'might': True, 'conclude': True, 'patches': True, 'unlike': True, 'classifiers': True, 'primarily': True, 'judgments': True, 'spatial': True, 'components': True, 'picture': True, 'pixels': True, 'oblivious': True, 'correlate': True, 'modifying': True, 'legitimate': True, 'adversarial': True, 'misclassifies': True, 'nonlinear': True, 'non-pattern': True, 'perturbations': True, 'possible': True, 'change': True, 'adversarially': True, 'pixel': True, 'vulnerable': True, 'manipulation': True, 'and/or': True, 'evasion': True, 'demonstrated': True, 'backdoors': True, 'placed': True, 'undetectably': True, 'spam': True, 'well-visible': True, 'posts': True, 'third': True, 'parties': True, 'data/software': True, 'transparency': True, 'white-box': True, 'assessments': True, 'validated': True, 'holdout': True, 'splits': True, 'conventionally': True, '2/3': True, '1/3': True, 'designation': True, 'evaluates': True, 'comparison': True, 'k-fold-cross-validation': True, 'randomly': True, 'partitions': True, 'k': True, 'experiments': True, 'performed': True, 'respectively': True, 'subset': True, 'remaining': True, 'k-1': True, 'cross-validation': True, 'bootstrap': True, 'replacement': True, 'assess': True, 'investigators': True, 'frequently': True, 'true': True, 'rate': True, 'tpr': True, 'tnr': True, 'similarly': True, 'false': True, 'fpr': True, 'fnr': True, 'ratios': True, 'reveal': True, 'numerators': True, 'denominators': True, 'total': True, 'operating': True, 'toc': True, 'shows': True, 'mentioned': True, 'receiver': True, 'roc': True, 'curve': True, 'auc': True, 'host': True, 'ethical': True, 'questions': True, 'datasets': True, 'exhibit': True, 'upon': True, 'digitizing': True, 'cultural': True, 'prejudices': True, '1988': True, 'uk': True, 'commission': True, 'racial': True, 'equality': True, 'st.': True, 'george': True, 'school': True, 'admissions': True, 'staff': True, 'denied': True, 'nearly': True, '60': True, 'candidates': True, 'women': True, 'non-european': True, 'sounding': True, 'names': True, 'job': True, 'hiring': True, 'policies': True, 'duplicating': True, 'scoring': True, 'applicants': True, 'includes': True, 'policing': True, 'geolitica': True, 'resulted': True, 'disproportionately': True, 'over-policing': True, 'low-income': True, 'minority': True, 'historical': True, 'crime': True, 'responsible': True, 'documentation': True, 'critical': True, 'blame': True, 'participation': True, 'vulnerability': True, 'cra': True, '2021': True, 'female': True, 'faculty': True, 'merely': True, '16.1': True, 'universities': True, 'world': True, 'furthermore': True, 'group': True, 'u.s.': True, 'resident': True, 'phd': True, 'graduates': True, '45': True, 'identified': True, '22.4': True, 'asian': True, '3.2': True, 'hispanic': True, '2.4': True, 'african': True, 'american': True, 'further': True, 'demonstrates': True, 'diversity': True, 'well-equipped': True, 'technical': True, 'rely': True, 'heavily': True, 'objectivity': True, 'languages': True, 'corpora': True, 'personal': True, 'seen': True, 'health': True, 'care': True, 'concerns': True, 'professionals': True, 'designed': True, 'public': True, 'income-generating': True, 'united': True, 'states': True, 'long-standing': True, 'dilemma': True, 'improving': True, 'profits': True, 'provide': True, 'patients': True, 'unnecessary': True, 'tests': True, 'medication': True, 'proprietary': True, 'owners': True, 'hold': True, 'stakes': True, 'diagnose': True, 'medicate': True, 'plan': True, 'recovery': True, 'paths': True, 'requires': True, 'mitigated': True, 'hardware': True, 'since': True, '2010s': True, 'advances': True, 'led': True, 'narrow': True, 'subdomain': True, 'graphic': True, 'gpus': True, 'ai-specific': True, 'enhancements': True, 'displaced': True, 'cpus': True, 'dominant': True, 'commercial': True, 'cloud': True, 'openai': True, 'largest': True, 'projects': True, 'alexnet': True, 'alphazero': True, '2017': True, '300,000-fold': True, 'increase': True, 'required': True, 'doubling-time': True, '3.4': True, 'months': True, 'neuromorphic/physical': True, 'physical': True, 'neuromorphic': True, 'electrically': True, 'adjustable': True, 'material': True, 'emulate': True, 'synapse': True, 'emphasize': True, 'reliance': True, 'opposed': True, 'software-based': True, 'applicable': True, 'memristor': True, 'resistance': True, 'embedded': True, 'sub-field': True, 'run': True, 'wearable': True, 'edge': True, 'microcontrollers': True, 'running': True, 'removes': True, 'transferring': True, 'storing': True, 'servers': True, 'henceforth': True, 'breaches': True, 'leaks': True, 'happening': True, 'minimizes': True, 'theft': True, 'intellectual': True, 'secrets': True, 'acceleration': True, 'approximate': True, 'pruning': True, 'quantization': True, 'distillation': True, 'low-rank': True, 'architecture': True, 'nas': True, 'parameter': True, 'sharing': True, 'few': True, 'suites': True, 'containing': True, 'free': True, 'open-source': True, 'editions': True, 'knime': True, 'rapidminer': True, 'ieee': True, 'aaai': True, 'conference': True, 'linguistics': True, 'acl': True, 'european': True, 'practice': True, 'international': True, 'biostatistics': True, 'cibb': True, 'icml': True, 'iclr': True, 'robots': True, 'iros': True, 'neurips': True, 'see': True, '–': True, 'automating': True, 'extremely': True, 'differentiable': True, 'force': True, 'list': True, 'publications': True, 'references': True, 'sources': True, 'domingos': True, 'pedro': True, 'september': True, '22': True, 'master': True, 'remake': True, 'our': True, 'books': True, 'isbn': True, '978-0465065707.': True, 'nils': True, '1998': True, 'synthesis': True, 'morgan': True, 'kaufmann': True, '978-1-55860-467-4.': True, 'archived': True, 'july': True, '2020.': True, 'retrieved': True, '18': True, 'november': True, '2019.': True, 'russell': True, 'stuart': True, 'j.': True, 'norvig': True, 'peter': True, '2003': True, '2nd': True, 'ed': True, 'upper': True, 'saddle': True, 'river': True, 'jersey': True, 'prentice': True, 'hall': True, '0-13-790395-2.': True, 'poole': True, 'david': True, 'mackworth': True, 'goebel': True, 'randy': True, 'york': True, 'oxford': True, 'university': True, 'press': True, '978-0-19-510270-3.': True, 'august': True, 'reading': True, 'links': True, 'quotations': True, 'wikiquote': True, 'mloss': True}\n",
            "\n",
            "Processed Text with Snowball stopwords and stemmer:\n",
            "machin learn ( ml ) field studi artifici intellig concern develop studi statist algorithm learn data general unseen data , thus perform task without explicit instruct . recent , artifici neural network abl surpass mani previous approach perform . ml find applic mani field , includ natur languag process , comput vision , speech recognit , email filter , agricultur , medicin . appli busi problem , known name predict analyt . although machin learn statist base , comput statist import sourc field 's\n",
            "\n",
            "Processed Text with WordNet Lemmatizer:\n",
            "machin learn ( ml ) field studi artifici intellig concern develop studi statist algorithm learn data gener unseen data , thu perform task without explicit instruct . recent , artifici neural network abl surpass mani previou approach perform . ml find applic mani field , includ natur languag process , comput vision , speech recognit , email filter , agricultur , medicin . appli busi problem , known name predict analyt . although machin learn statist base , comput statist import sourc field 's met\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes without Pre-processing:"
      ],
      "metadata": {
        "id": "9F2HCtj0JluO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'geographic' and 'non-geographic' are classes\n",
        "geographic_class = \"location\"\n",
        "non_geographic_class = \"non-location\"\n",
        "\n",
        "# Training data for Naive Bayes on Bag of Words without pre-processing\n",
        "training_data_bow_raw = [\n",
        "    (get_wiki_content_with_terms(\"Florence\", location_terms)[0].lower(), geographic_class),\n",
        "    (get_wiki_content_with_terms(\"Machine learning\", non_location_terms)[0].lower(), non_geographic_class),\n",
        "]\n",
        "\n",
        "# Tokenizer function for raw text\n",
        "def tokenize_raw_text_bow(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Feature extraction function for raw text\n",
        "def extract_features_raw_bow(text):\n",
        "    return {word: True for word in tokenize_raw_text_bow(text)}\n",
        "\n",
        "# Prepare the training set without pre-processing for Bag of Words\n",
        "training_set_bow_raw = [(extract_features_raw_bow(text), label) for (text, label) in training_data_bow_raw]\n",
        "\n",
        "# Train the Naive Bayes classifier without pre-processing for Bag of Words\n",
        "nb_classifier_bow_raw = NaiveBayesClassifier.train(training_set_bow_raw)\n",
        "\n",
        "# Usage without pre-processing for Bag of Words\n",
        "test_text_bow_raw = get_wiki_content_with_terms(\"Florence\", location_terms)[0].lower()\n",
        "test_features_bow_raw = extract_features_raw_bow(test_text_bow_raw)\n",
        "classification_bow_raw = nb_classifier_bow_raw.classify(test_features_bow_raw)\n",
        "\n",
        "print(f\"\\nPredicted class for the test text without pre-processing for Bag of Words: {classification_bow_raw}\")\n",
        "\n",
        "test_text_bow_raw = get_wiki_content_with_terms(\"Data Science\", non_location_terms)[0].lower()\n",
        "test_features_bow_raw = extract_features_raw_bow(test_text_bow_raw)\n",
        "classification_bow_raw = nb_classifier_bow_raw.classify(test_features_bow_raw)\n",
        "\n",
        "print(f\"Predicted class for the test text without pre-processing for Bag of Words: {classification_bow_raw}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgcc9O1vIE8F",
        "outputId": "84423749-316c-44da-95ea-a888dbbf9559"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class for the test text without pre-processing for Bag of Words: location\n",
            "Predicted class for the test text without pre-processing for Bag of Words: non-location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes with Pre-processing:\n"
      ],
      "metadata": {
        "id": "GpiQmdmiKzZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data for Naive Bayes with Snowball stop words and Snowball Stemmer\n",
        "training_data_bow_snowball = [\n",
        "    (process_text_with_terms(get_wiki_content_with_terms(\"Florence\", location_terms)[0], location_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer).lower(), geographic_class),\n",
        "    (process_text_with_terms(get_wiki_content_with_terms(\"Machine learning\", non_location_terms)[0], non_location_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer).lower(),\n",
        "                                non_geographic_class),\n",
        "]\n",
        "\n",
        "# Tokenizer function for preprocessed text\n",
        "def tokenize_preprocessed_text_bow(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Feature extraction function for preprocessed text\n",
        "def extract_features_preprocessed_bow(text):\n",
        "    return {word: True for word in tokenize_preprocessed_text_bow(text)}\n",
        "\n",
        "# Prepare the training set with Snowball stop words and Snowball Stemmer for Bag of Words\n",
        "training_set_bow_snowball = [(extract_features_preprocessed_bow(text), label) for (text, label) in training_data_bow_snowball]\n",
        "\n",
        "# Train the Naive Bayes classifier with Snowball stop words and Snowball Stemmer for Bag of Words\n",
        "nb_classifier_bow_snowball = NaiveBayesClassifier.train(training_set_bow_snowball)\n",
        "\n",
        "# Usage with Snowball stop words and Snowball Stemmer for Bag of Words\n",
        "test_text_bow_snowball = process_text_with_terms(get_wiki_content_with_terms(\"Florence\", location_terms)[0], location_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer).lower()\n",
        "test_features_bow_snowball = extract_features_preprocessed_bow(test_text_bow_snowball)\n",
        "classification_bow_snowball = nb_classifier_bow_snowball.classify(test_features_bow_snowball)\n",
        "\n",
        "print(f\"\\nPredicted class for the preprocessed test text with Snowball stop words and Snowball Stemmer for Bag of Words: {classification_bow_snowball}\")\n",
        "\n",
        "test_text_bow_snowball = process_text_with_terms(get_wiki_content_with_terms(\"Data Science\", non_location_terms)[0], non_location_terms, stopwords_set=set(stopwords.words('english')),\n",
        "                                                 text_stemmer=snowball_stemmer).lower()\n",
        "test_features_bow_snowball = extract_features_preprocessed_bow(test_text_bow_snowball)\n",
        "classification_bow_snowball = nb_classifier_bow_snowball.classify(test_features_bow_snowball)\n",
        "\n",
        "print(f\"Predicted class for the preprocessed test text with Snowball stop words and Snowball Stemmer for Bag of Words: {classification_bow_snowball}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZUheUeVKykn",
        "outputId": "7694fe94-d66d-445c-db2b-fd2bbf4fe44b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class for the preprocessed test text with Snowball stop words and Snowball Stemmer for Bag of Words: location\n",
            "Predicted class for the preprocessed test text with Snowball stop words and Snowball Stemmer for Bag of Words: non-location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "Vt-RUBdKJzaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data for Logistic Regression with pre-processing from Naive Bayes\n"
      ],
      "metadata": {
        "id": "FLKPgusQEEe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training data for Logistic Regression with pre-processing from Naive Bayes\n",
        "\n",
        "\n",
        "# Training data for Logistic Regression with pre-processing from Naive Bayes\n",
        "logistic_train_data_preprocessed = [\n",
        "    (process_text_with_terms(get_wiki_content_with_terms(\"Florence\", location_terms)[0], location_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer).lower(),\n",
        "                            geographic_class),\n",
        "    (process_text_with_terms(get_wiki_content_with_terms(\"Machine learning\", non_location_terms)[0], non_location_terms, stopwords_set=set(stopwords.words('english')), text_stemmer=snowball_stemmer).lower(),\n",
        "                            non_geographic_class),\n",
        "]\n",
        "\n",
        "# Tokenizer function for preprocessed text (same as used for Naive Bayes)\n",
        "def tokenize_preprocessed_logistic_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Feature extraction function for preprocessed text (same as used for Naive Bayes)\n",
        "def extract_preprocessed_logistic_features(text):\n",
        "    return ' '.join(tokenize_preprocessed_logistic_text(text))\n",
        "\n",
        "# Prepare the training set for Logistic Regression with pre-processing\n",
        "logistic_train_set_preprocessed = [(extract_preprocessed_logistic_features(text), label) for (text, label) in logistic_train_data_preprocessed]\n",
        "\n",
        "# Create TF-IDF vectors from the training set for Logistic Regression with pre-processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Separate features (X) and labels (y) for Logistic Regression with pre-processing\n",
        "X_train_logistic_preprocessed = [text for (text, _) in logistic_train_set_preprocessed]\n",
        "y_train_logistic_preprocessed = [label for (_, label) in logistic_train_set_preprocessed]\n",
        "\n",
        "# Create TF-IDF vectors from the training set for Logistic Regression with pre-processing\n",
        "tfidf_vectorizer_logistic_preprocessed = TfidfVectorizer()\n",
        "X_train_tfidf_logistic_preprocessed = tfidf_vectorizer_logistic_preprocessed.fit_transform(X_train_logistic_preprocessed)\n",
        "\n",
        "# Train the Logistic Regression classifier with pre-processing\n",
        "logistic_regressor_preprocessed = LogisticRegression()\n",
        "logistic_regressor_preprocessed.fit(X_train_tfidf_logistic_preprocessed, y_train_logistic_preprocessed)\n",
        "\n",
        "# Usage for Logistic Regression with pre-processing\n",
        "test_text_logistic_preprocessed = process_text_with_terms(get_wiki_content_with_terms(\"Florence\", location_terms)[0], location_terms, stopwords_set=set(stopwords.words('english')),\n",
        "                                                          text_stemmer=snowball_stemmer).lower()\n",
        "test_features_logistic_preprocessed = tfidf_vectorizer_logistic_preprocessed.transform([extract_preprocessed_logistic_features(test_text_logistic_preprocessed)])\n",
        "predicted_class_logistic_preprocessed = logistic_regressor_preprocessed.predict(test_features_logistic_preprocessed)\n",
        "\n",
        "print(f\"\\nPredicted class for the test text using Logistic Regression with pre-processing: {predicted_class_logistic_preprocessed[0]}\")\n",
        "\n",
        "test_text_logistic_preprocessed = process_text_with_terms(get_wiki_content_with_terms(\"Data Science\", non_location_terms)[0], non_location_terms, stopwords_set=set(stopwords.words('english')),\n",
        "                                                          text_stemmer=snowball_stemmer).lower()\n",
        "test_features_logistic_preprocessed = tfidf_vectorizer_logistic_preprocessed.transform([extract_preprocessed_logistic_features(test_text_logistic_preprocessed)])\n",
        "predicted_class_logistic_preprocessed = logistic_regressor_preprocessed.predict(test_features_logistic_preprocessed)\n",
        "\n",
        "print(f\"Predicted class for the test text using Logistic Regression with pre-processing: {predicted_class_logistic_preprocessed[0]}\")\n",
        "\n",
        "# Training data for Logistic Regression without pre-processing\n",
        "logistic_train_data_raw = [\n",
        "    (get_wiki_content_with_terms(\"Florence\", location_terms)[0].lower(), geographic_class),\n",
        "    (get_wiki_content_with_terms(\"Machine learning\", non_location_terms)[0].lower(), non_geographic_class),\n",
        "]\n",
        "\n",
        "# Tokenizer function for raw text (same as used for Naive Bayes)\n",
        "def tokenize_raw_logistic_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Feature extraction function for raw text (same as used for Naive Bayes)\n",
        "def extract_raw_logistic_features(text):\n",
        "    return ' '.join(tokenize_raw_logistic_text(text))\n",
        "\n",
        "# Prepare the training set for Logistic Regression without pre-processing\n",
        "logistic_train_set_raw = [(extract_raw_logistic_features(text), label) for (text, label) in logistic_train_data_raw]\n",
        "\n",
        "# Create TF-IDF vectors from the training set for Logistic Regression without pre-processing\n",
        "X_train_logistic_raw = [text for (text, _) in logistic_train_set_raw]\n",
        "y_train_logistic_raw = [label for (_, label) in logistic_train_set_raw]\n",
        "\n",
        "tfidf_vectorizer_logistic_raw = TfidfVectorizer()\n",
        "X_train_tfidf_logistic_raw = tfidf_vectorizer_logistic_raw.fit_transform(X_train_logistic_raw)\n",
        "\n",
        "# Train the Logistic Regression classifier without pre-processing\n",
        "logistic_regressor_raw = LogisticRegression()\n",
        "logistic_regressor_raw.fit(X_train_tfidf_logistic_raw, y_train_logistic_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "wJTNHSk-IE-y",
        "outputId": "ebfe6bf0-3a49-45ed-b1ec-aa5489929f21"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class for the test text using Logistic Regression with pre-processing: location\n",
            "Predicted class for the test text using Logistic Regression with pre-processing: non-location\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage for Logistic Regression without pre-processing\n"
      ],
      "metadata": {
        "id": "5Aer7p19EI7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_logistic_raw = get_wiki_content_with_terms(\"Florence\", location_terms)[0].lower()\n",
        "test_features_logistic_raw = tfidf_vectorizer_logistic_raw.transform([extract_raw_logistic_features(test_text_logistic_raw)])\n",
        "predicted_class_logistic_raw = logistic_regressor_raw.predict(test_features_logistic_raw)\n",
        "\n",
        "print(f\"\\nPredicted class for the test text using Logistic Regression without pre-processing: {predicted_class_logistic_raw[0]}\")\n",
        "\n",
        "test_text_logistic_raw = get_wiki_content_with_terms(\"Data Science\", non_location_terms)[0].lower()\n",
        "test_features_logistic_raw = tfidf_vectorizer_logistic_raw.transform([extract_raw_logistic_features(test_text_logistic_raw)])\n",
        "predicted_class_logistic_raw = logistic_regressor_raw.predict(test_features_logistic_raw)\n",
        "\n",
        "print(f\"Predicted class for the test text using Logistic Regression without pre-processing: {predicted_class_logistic_raw[0]}\")"
      ],
      "metadata": {
        "id": "7Dya_XgQK8p6",
        "outputId": "260575ab-ac3a-4832-d0ec-30494ecdfb5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class for the test text using Logistic Regression without pre-processing: location\n",
            "Predicted class for the test text using Logistic Regression without pre-processing: non-location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 2"
      ],
      "metadata": {
        "id": "J3oS1frbEWv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Faker library for generating dummy text\n",
        "!pip install faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uI_e1ecG1av",
        "outputId": "734c1499-e094-4595-ec98-f2033e021692"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (25.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from faker import Faker"
      ],
      "metadata": {
        "id": "ajRk30bIEcT1"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK's Punkt tokenizer models\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvcl1xE4GDjm",
        "outputId": "0084f802-5539-418f-8b7e-12557204b923"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_WINDOW_LIMIT = 4000  # The maximum number of tokens allowed in the context window\n",
        "\n",
        "def measure_length(text):\n",
        "    \"\"\"\n",
        "    Measure the length of the text in terms of the number of tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    return len(tokens)\n",
        "\n",
        "def proportional_length(length1, length2, target_length):\n",
        "    \"\"\"\n",
        "    Compute the target lengths for the summaries proportionally based on the original lengths.\n",
        "    \"\"\"\n",
        "    proportion = target_length / (length1 + length2)\n",
        "    return int(length1 * proportion), int(length2 * proportion)\n",
        "\n",
        "def slice_document(text, max_tokens):\n",
        "    \"\"\"\n",
        "    Slice the document into chunks that fit within the context window limit.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        yield ' '.join(tokens[i:i + max_tokens])\n",
        "\n",
        "def summarize_text(text, target_length=None):\n",
        "    \"\"\"\n",
        "    Summarize the text to the target length in terms of tokens. If no target length is provided,\n",
        "    return the text as it is.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    if target_length and len(tokens) > target_length:\n",
        "        return ' '.join(tokens[:target_length])\n",
        "    return text\n",
        "\n",
        "def collate_summaries(summaries):\n",
        "    \"\"\"\n",
        "    Collate all the summaries into a single summary.\n",
        "    \"\"\"\n",
        "    return ' '.join(summaries)\n",
        "\n",
        "def hierarchical_summarization(document, max_tokens):\n",
        "    \"\"\"\n",
        "    Perform hierarchical summarization on the document to ensure the summary fits\n",
        "    within the context window limit.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    # Slice the document and summarize each slice\n",
        "    for slice in slice_document(document, max_tokens):\n",
        "        summaries.append(summarize_text(slice, max_tokens))\n",
        "\n",
        "    # Collate all summaries into a single summary\n",
        "    combined_summary = collate_summaries(summaries)\n",
        "\n",
        "    # If the combined summary exceeds the context window, further summarize it\n",
        "    while measure_length(combined_summary) > CONTEXT_WINDOW_LIMIT:\n",
        "        combined_summary = summarize_text(combined_summary, CONTEXT_WINDOW_LIMIT // 2)\n",
        "\n",
        "    return combined_summary\n",
        "\n",
        "def save_document(summary, filename):\n",
        "    \"\"\"\n",
        "    Save the summarized document to a file.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(summary)"
      ],
      "metadata": {
        "id": "M4KSoVKzGFSl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Faker for generating dummy documents\n",
        "fake = Faker()\n",
        "\n",
        "# Generate two large documents with dummy text\n",
        "document1 = \" \".join(fake.paragraphs(nb=200))  # First document with 200 paragraphs\n",
        "document2 = \" \".join(fake.paragraphs(nb=500))  # Second document with 500 paragraphs\n",
        "\n",
        "# Measure the lengths of the documents in tokens\n",
        "length1 = measure_length(document1)\n",
        "length2 = measure_length(document2)\n",
        "\n",
        "# Compute the target lengths proportionally based on the total target length (4000 tokens)\n",
        "target_length1, target_length2 = proportional_length(length1, length2, CONTEXT_WINDOW_LIMIT)\n",
        "\n",
        "# Perform hierarchical summarization on both documents\n",
        "summary1 = hierarchical_summarization(document1, target_length1)\n",
        "summary2 = hierarchical_summarization(document2, target_length2)\n",
        "\n",
        "# Save the summarized documents to files\n",
        "save_document(summary1, 'summary1.txt')\n",
        "save_document(summary2, 'summary2.txt')\n",
        "\n",
        "print(\"Summarization complete and saved.\")"
      ],
      "metadata": {
        "id": "M-jqoJIzG_IE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3129df1c-a0dd-4ec7-9bd0-328e40317f7d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization complete and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_summary(filename, num_sentences=5):\n",
        "    \"\"\"\n",
        "    Read the saved summary from the file and display the first few sentences.\n",
        "    \"\"\"\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "        sentences = sent_tokenize(content)\n",
        "        # Display the first few sentences of the summary\n",
        "        print(f\"--- {filename} ---\")\n",
        "        print(' '.join(sentences[:num_sentences]))\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Display the first few sentences of the saved summaries\n",
        "display_summary('summary1.txt')\n",
        "display_summary('summary2.txt')"
      ],
      "metadata": {
        "id": "XsdlTq-SMGB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500474ad-9a03-47cd-8f6d-ff6b43ec6eeb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- summary1.txt ---\n",
            "Summer five clearly career oil also . Quickly plant nothing end . Treat knowledge say six shoulder . Ability trouble southern view everyone whatever garden . Dinner personal democratic consumer her house .\n",
            "\n",
            "\n",
            "--- summary2.txt ---\n",
            "Huge true phone share always . Whom and order similar leader why thousand environment . Himself easy entire prove leave local they . Discussion group pretty receive . Main middle international .\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}